{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_432_Train_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.22.4"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JAaron93/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/module2-Train/LS_DS_422_Train_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Train Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Please build a baseline classification model then run a few experiments with different optimizers and learning rates. \n",
        "\n",
        "*Don't forgot to switch to GPU on Colab!*\n",
        "\n",
        "\n",
        "------\n",
        "\n",
        "# Objective \n",
        "\n",
        "We are going to run a few experiments today\n",
        "\n",
        "- Train a model with and without normalized data and investigate the weight values and learning outcomes\n",
        "- Train a model with varying values for batch_size, learning_rate, and optimizers\n",
        "\n",
        "We are essentially running mannual gridsearches on our models. In module 3, we'll learn a few different ways to automate gridseach for deep learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULd6reIQEeM0"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, ReLU\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.activations import relu\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLQxwyjqEeM1"
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol-7Oxv5EeM2"
      },
      "source": [
        "### Load data\n",
        "\n",
        "- Don't normalize your data just yet!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJsIsrvp7O3e"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Loads in the quickdraw dataset that has been sampled to 10 classes with 10000 observations per class as pre-split features (X) and target (Y)\n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = tf.keras.utils.get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "\n",
        "    X = data['arr_0']\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGUPTEjwEeM3",
        "outputId": "2617fb20-d684-44a4-bdf7-34f9903366a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This function works like a tensorflow/numpy hybrid of sk-learn's train_test_split function before reading in data\n",
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\n",
            "25427968/25421363 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EGYlaSCEeM4",
        "outputId": "77f41a59-fbb0-4154-c809-7f9536d0e96d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Node labels for our output layer we'll be using later\n",
        "n_labels = len(np.unique(y_train))\n",
        "n_labels"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-6PxI6H5__2"
      },
      "source": [
        "----\n",
        "### Write a Model Function\n",
        "- Write a function called `create_model` which returns a compiled TensorFlow Keras Sequential Model suitable for classifying the QuickDraw-10 dataset. \n",
        "\n",
        "Your function `create_model` should accept the following parameters\n",
        "\n",
        "- Learning Rate `lr`\n",
        "- Optimizer `opt`\n",
        "\n",
        "\n",
        "Build a model with the following architecture and parameter values\n",
        "\n",
        "- Use `1 hidden layer` \n",
        "- Use `sigmoid` activation function in hidden layer\n",
        "- Use `250 nodes` in hidden layer \n",
        "- Use `10 nodes` in output layer\n",
        "- Use `softmax` activation fucntion in output layer\n",
        "- Use `sparse_categorical_crossentropy` loss function\n",
        "- Use `accuracy` as your metric \n",
        "\n",
        "We will use this function to build all the models that we'll need to run our experiments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "nEREYT-3wI1f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6692fdabca8804cd50380fd4b9a56169",
          "grade": false,
          "grade_id": "cell-355125ca910bfedb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "def create_model(lr=.01, opt=\"adam\"):\n",
        "    \"\"\"\n",
        "    \n",
        "    Build and returns a complies Keras model.  \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    lr: float\n",
        "        Learing rate parameter used for Stocastic Gradient Descent \n",
        "        \n",
        "    opt: string\n",
        "        Name of optimizer to use\n",
        "        Valid options are \"adam\" and \"sgd\"\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "        A complied keras model \n",
        "    \"\"\"\n",
        "\n",
        "    if opt == \"adam\":\n",
        "        opt = Adam(learning_rate=lr)\n",
        "    elif opt == 'sgd':\n",
        "        opt = SGD(learning_rate=lr)\n",
        "    elif opt == 'rmsprop':\n",
        "        opt = RMSprop(learning_rate=lr)\n",
        "    elif opt == 'adadelta':\n",
        "        opt = Adadelta(learning_rate=lr)\n",
        "    elif opt == 'adagrad':\n",
        "        opt = Adagrad(learning_rate=lr)\n",
        "    elif opt == \"adamax\":\n",
        "        opt = Adamax(learning_rate=lr)\n",
        "    elif opt == \"nadam\":\n",
        "        opt = Nadam(learning_rate=lr)\n",
        "    elif opt == \"Ftrl\":\n",
        "         opt = Ftrl(learning_rate=lr)\n",
        "    else:\n",
        "        print (\"{} is not a valid option. Defaulting to Adam optimizer\".format(opt))\n",
        "        opt = Adam(learning_rate=lr)\n",
        "\n",
        "    # build model here\n",
        "    # YOUR CODE HERE\n",
        "    # Instantiating a Sequential class model as an object by assigning it\n",
        "    model = Sequential()\n",
        "\n",
        "    # Adding 1 hidden layer\n",
        "    model.add(Dense(250,\n",
        "                    input_dim=784,\n",
        "                    activation=\"sigmoid\"))\n",
        "    # Adding output layer\n",
        "    model.add(Dense(n_labels,\n",
        "                    activation=\"softmax\"))\n",
        "    # Compiling these layers into our model\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4IRVsckEeM7"
      },
      "source": [
        "# a check on model architecture\n",
        "model = create_model()\n",
        "n_layers = len(model.get_config()[\"layers\"])\n",
        "output_act_funct =  model.get_config()[\"layers\"][-1][\"config\"][\"activation\"]\n",
        "\n",
        "assert n_layers == 3, \"You should have an input, one hidden, and an output layer\"\n",
        "assert output_act_funct == \"softmax\", \"Output act funct should be softmax\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2ZfVhG1EeM7"
      },
      "source": [
        "------\n",
        "\n",
        "# Experiment #1: How does normalized input data affect our model's learning outcome?\n",
        "\n",
        "In this experiment we are going to answer the above question by training identifical models on a normalized data set and on a non-normalized data set. \n",
        "\n",
        "Then we will \n",
        "\n",
        "- Analyze the trained weight values of our model \n",
        "- Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPkUkQUjEeM8"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIiXIcrYEeM9"
      },
      "source": [
        "### Fit Model on Non-Normalized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Jp-QK_Xugv"
      },
      "source": [
        "# Had to disable eager execution. Tensorboard wouldn't run otherwise\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnVkOvjLEeM9"
      },
      "source": [
        "# fit model on non-normalized data\n",
        "\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = os.path.join(\"logs\", f\"No_Normalization-{now}\")\n",
        "tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
        "\n",
        "model = create_model(lr=.001, opt=\"adam\")\n",
        "\n",
        "model.fit(X_train, y_train, \n",
        "          validation_data=(X_test, y_test),\n",
        "          workers=-2, \n",
        "          epochs=10, \n",
        "          batch_size=32, \n",
        "          verbose=1, \n",
        "          callbacks=[tensorboard])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBTfQw96EeM-"
      },
      "source": [
        "---------\n",
        "\n",
        "### Fit Model on Normalized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ccc6ed9c1e2c04f77eac26d557236dad",
          "grade": false,
          "grade_id": "cell-2792e5f1fbf02c67",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "B34PuwpVEeM_"
      },
      "source": [
        "# Normalize your training and test sets \n",
        "# save normalized data to X_train_scaled and X_test_scaled\n",
        "\n",
        "# YOUR CODE HERE\n",
        "X_train_scaled = X_train/X_train.max()\n",
        "X_test_scaled = X_test/X_test.max()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Zguze6EeM_",
        "outputId": "dddceded-d6a2-4d9f-98e1-8f059b473fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        }
      },
      "source": [
        "# train model on normalized data\n",
        "\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = os.path.join(\"logs\", f\"Normalization-{now}\")\n",
        "tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
        "\n",
        "norm_model = create_model(lr=.001, opt=\"adam\")\n",
        "\n",
        "norm_model.fit(X_train_scaled, y_train, \n",
        "          validation_data=(X_test_scaled, y_test),\n",
        "          workers=-2, \n",
        "          epochs=10, \n",
        "          batch_size=32, \n",
        "          verbose=1, \n",
        "          callbacks=[tensorboard])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 75000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "\r   32/75000 [..............................] - ETA: 1:46 - loss: 2.3596 - accuracy: 0.0938"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "74336/75000 [============================>.] - ETA: 0s - loss: 0.7714 - accuracy: 0.7711"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-65572e9ead60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m           callbacks=[tensorboard])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    426\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4054\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 4055\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   4056\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4057\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1480\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: You must feed a value for placeholder tensor 'dense_2_input' with dtype float and shape [?,784]\n\t [[{{node dense_2_input}}]]\n\t [[metrics_2/accuracy/Identity/_175]]\n  (1) Invalid argument: You must feed a value for placeholder tensor 'dense_2_input' with dtype float and shape [?,784]\n\t [[{{node dense_2_input}}]]\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iCdMz0HEeNA"
      },
      "source": [
        "### Each layer is labeled\n",
        "\n",
        "Take note of the label for each layer in the network. It is these labels that will help you identify the corresponding bias and weight distribtuions on tensorboard. \n",
        "\n",
        "Assuming that you've ran `create_model` 3 times: once for the model check, once to create `model`, and once to create `norm_model`:\n",
        "\n",
        "The name of the layers for `model` should be \n",
        "- dense_2\n",
        "- dense_3\n",
        "\n",
        "The name of the layers for `norm_model` should be \n",
        "- dense_4\n",
        "- dense_5\n",
        "\n",
        "\n",
        "If you keep retraining one or both of these models, tensorflow will increment the integer used in the layer names.  But that doesn't really matter, just take notice of the layer names so you can find their corresponding bias and weight distribtuions in tensorboard.\n",
        "\n",
        "**Protip:** If you want to reset the integer incrementation that tensorflow uses, you'll need to restart your notebook's kernal. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14mjCd3oEeNA"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KWhuF3LEeNB"
      },
      "source": [
        "norm_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKBxxrupEeNB"
      },
      "source": [
        "### Tensorboard \n",
        "\n",
        "- Run the cell below to launch tensorboard \n",
        "- Click on the `SCALARS` tab to see plots that compare the loss and accuracy between the two models\n",
        "- Cick on the `HISTOGRAMS` tab to see the distribution of the learned weights "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP7-5foNEeNC"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ZG8eNLEeNC"
      },
      "source": [
        "### Tensorboard \n",
        "\n",
        "Check out the loss and accuracy plots on the `SCALARS` tab. \n",
        "\n",
        "What you should see is that the accuracy is much higher for the model that was given normalized data; conversely, the loss is much lower for the model that was given normalized data. \n",
        "\n",
        "Recall that what we are doing whenever training a model is adjusting the value of the bias and weights in each layer. For simplicity of analysis, we only trained two layers: a hidden layer and the output layer. \n",
        "\n",
        "Now click on the `HISTOGRAM` tab. \n",
        "\n",
        "You should see both of your model's layer names. \n",
        "\n",
        "### Hidden Layer Distributions\n",
        "\n",
        "Collapse the charts that correspond to the output layer so only the distributions for the weights and bias in the hidden layer are showing. (i.e. Only expand `dense_2` and `dense_4`). \n",
        "\n",
        "Also don't be confused by the word `kernel`, that's just the word that Tensorflow uses instead of weights. So, to be clear, **the kernal distributions are the weight values.** \n",
        "\n",
        "The `bias` distributions are the bias values. \n",
        "\n",
        "You should see 10 distributions stacked next to each other, **one distribution per epoch.**\n",
        "\n",
        "The distribuion in the far back corresponds to the weight values at epoch 1 (tensorflow starts the count at 0, like the index for a list). The distribution at the very front corresponds to the weight values at the 10th epoch (tensorflow indexing show 9 instead of 10).\n",
        "\n",
        "Notice how the shape of the distribution changes accross epochs? That's because their **values are being updated via Gradient Descent.** \n",
        "\n",
        "The distributions that you see are direclty responsible for the validation accuracy of our models. The reason why they look different between the two models is because one model was given normalize data and one wasn't. So you can conclude that the weight distributions in `dense_4` produce a higher validation accuracy than the weight distributions in `dense_2`. \n",
        "\n",
        "Now it's time to analyze those weight values more closely. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUAma92PEeND"
      },
      "source": [
        "----------\n",
        "### Analyze Weights in Each Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPynnuxzEeNE"
      },
      "source": [
        "# get the final bias and weight matrices for model\n",
        "layer = model.get_layer(name=\"dense_2\")\n",
        "bias, weights = layer.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkH9gjTMEeNE"
      },
      "source": [
        "# get the final  value bias and weight magrices for norm_moel\n",
        "layer = norm_model.get_layer(name=\"dense_4\")\n",
        "bias_norm, weights_norm = layer.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnNRBU1zEeNF"
      },
      "source": [
        "# this line of code should not throw an error if the number of weights is the same for the hidden layer of both models\n",
        "# this line of code is known as a Unit Test \n",
        "assert weights.shape[0] == weights_norm.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVqvULuzEeNF"
      },
      "source": [
        "### Initial Weight Values\n",
        "\n",
        "By default, Keras dense layers randomly initialize the weight values using [**GlorotUniform**](https://keras.io/api/layers/initializers/). \n",
        "\n",
        "The cell below is sampling values from the GlorotUniform distribution. Let's sample from the GlorotUniform distribution and plot it in order to get a sense of the initial distribution of our weights - before Gradient Descent starts upading their values at training time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKM4afl3EeNG"
      },
      "source": [
        "# let's take 250 random samples form the GlorotUniform\n",
        "# because they are random samples their exact values might have been a little different for model and norm_model - but we will assume that they were not statistically different \n",
        "# 250 because that's how many weights are in the hidden layer for both of our models\n",
        "initializer = tf.keras.initializers.GlorotUniform(seed=1234)\n",
        "initial_weight_values = initializer(shape=(1, 250))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWo6VJKJEeNG"
      },
      "source": [
        "plt.figure(figsize=(20,6))\n",
        "plt.title(\"Learned Weights in Hidden Layers\")\n",
        "plt.xlabel(\"Weight Values\")\n",
        "plt.grid()\n",
        "\n",
        "# by setting density=True, we are transforming our plots into probability distributions \n",
        "plt.hist(weights, bins=20, alpha=0.5, label=\"weights from model\", density=True);\n",
        "plt.hist(weights_norm, bins=20, alpha=0.5, label=\"weights from norm_model\", density=True);\n",
        "plt.hist(initial_weight_values, bins=20, alpha=0.5, label=\"initial weight values\", density=True);\n",
        "plt.legend(fontsize=\"x-large\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24YxbBYEEeNH"
      },
      "source": [
        "### Observations\n",
        "\n",
        "Your plot should have 3 distributions\n",
        "\n",
        "- weights from model trained on non-normalized data\n",
        "- weights from model trained on normalized data\n",
        "- initial weight values sampled from a Glorot Uniform distributions \n",
        "\n",
        "Use the plot to answer the following questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtXfSfcVEeNH"
      },
      "source": [
        "**Comparing the initial weights with weights_from_model, what was the effect of not using normalized data?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "500ff9c3902f635e0a9aafa64e2cc95d",
          "grade": true,
          "grade_id": "cell-4d6c92df9d105c46",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "IFTwXQ5NEeNI"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58uG8pxEeNI"
      },
      "source": [
        "**Comparing the initial weights with weights_from_norm_model, what was the effect of using normalized data?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "23d7012c3d7ce759e5389a0dfee1892a",
          "grade": true,
          "grade_id": "cell-614992ba50bf54c4",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "6WzHw4G7EeNJ"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSrYp1HVEeNJ"
      },
      "source": [
        "**Using your understand of how Gradient Descent works, why do you think that the distributions between weights_from_model and weights_from_norm_model look so different?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "52e82163ac0a8d8b47bc613199650fe2",
          "grade": true,
          "grade_id": "cell-598a597c991950f8",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "JAV2EDf7EeNK"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOnZ6yI8EeNK"
      },
      "source": [
        "----\n",
        "\n",
        "# Additional Experiments\n",
        "\n",
        "The previous experiment demonstrated the importance of normalizing our data in order to maximize model accuracy. In the next few experiments, we are going to explore the effect that certain values for Batch Size, Learning Rate, and different Optimizers have on model accuracy. \n",
        "\n",
        "Using our **create_model** model building function, conduct the following experiments. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0pCkh8C7eGL"
      },
      "source": [
        "### Experiment with Batch Size\n",
        "* Run 5 experiments with various batch sizes of your choice. \n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against your model's performance yesterday. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b-r70o8p2Dm"
      },
      "source": [
        "### Experiment with Learning Rate\n",
        "* Run 5 experiments with various learning rate magnitudes: 1, .1, .01, .001, .0001.\n",
        "* Use the \"best\" batch size from the previous experiment\n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SA144xx8Luf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxMtSRhV9Q7I"
      },
      "source": [
        "### Experiment with different Optimizers\n",
        "* Run 5 experiments with various optimizers available in TensorFlow. See list [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
        "* Visualize the results\n",
        "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday.\n",
        "* Repeat the experiment combining Learning Rate and different optimizers. Does the best performing model change? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujLuzdNA91ip"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwlRJSfBlCvy"
      },
      "source": [
        "------\n",
        "\n",
        "## Stretch Goals: \n",
        "\n",
        "- On the learning rate experiments, implement [EarlyStopping](https://keras.io/api/callbacks/early_stopping/)\n",
        "- Review the math of Gradient Descent. "
      ]
    }
  ]
}